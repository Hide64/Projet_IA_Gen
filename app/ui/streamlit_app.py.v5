# -*- coding: utf-8 -*-
import os
import requests
import streamlit as st
import psycopg2
import pandas as pd
import numpy as np
import json
from psycopg2.extras import RealDictCursor

# ===============================
# CONFIGURATION
# ===============================
PG_HOST = os.getenv("POSTGRES_HOST", "postgres")
PG_DB   = os.getenv("POSTGRES_DB", "videotheque")
PG_USER = os.getenv("POSTGRES_USER", "postgres")
PG_PWD  = os.getenv("POSTGRES_PASSWORD", "postgres")
PG_PORT = int(os.getenv("POSTGRES_PORT", "5432"))

DEFAULT_USER_ID = int(os.getenv("APP_USER_ID", "1"))
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b")
EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text")

# ===============================
# SQL QUERIES
# ===============================

# Profil bas√© sur les notes pass√©es
SQL_GENRE_PROFILE = """
WITH rated_seen AS (
  SELECT film_id, rating_10 FROM user_film
  WHERE user_id = %(user_id)s AND status = 'SEEN' AND rating_10 IS NOT NULL
)
SELECT fg.genre_id, AVG(rs.rating_10) AS avg_rating
FROM rated_seen rs JOIN film_genre fg ON fg.film_id = rs.film_id
GROUP BY fg.genre_id;
"""

# Recherche hybride : Synopsis (film_embedding) + Commentaires (comment_embedding)
SQL_HYBRID_SEARCH = """
WITH sim_comments AS (
    SELECT uc.film_id, (1.0 - (ce.embedding <=> %(qvec)s::vector)) AS score
    FROM comment_embedding ce 
    JOIN user_comment uc ON uc.comment_id = ce.comment_id
    WHERE uc.user_id = %(user_id)s
),
sim_overviews AS (
    SELECT film_id, (1.0 - (embedding <=> %(qvec)s::vector)) AS score
    FROM film_embedding
)
SELECT film_id, MAX(score) as semantic_score
FROM (SELECT * FROM sim_comments UNION ALL SELECT * FROM sim_overviews) combined
GROUP BY film_id
ORDER BY semantic_score DESC LIMIT 20;
"""

# Liste des films non vus (Candidats)
SQL_CANDIDATES = """
WITH last_seen_ev AS (
  SELECT user_id, film_id, MAX(watched_at)::date AS last_seen
  FROM watch_event WHERE user_id = %(user_id)s GROUP BY user_id, film_id
)
SELECT DISTINCT
  f.film_id, f.title, f.year, f.runtime_min,
  COALESCE(uf.status::text, 'WANT') AS status,
  COALESCE(uf.last_seen_at, ls.last_seen) AS last_seen_at
FROM film f
JOIN film_source fs ON fs.film_id = f.film_id
LEFT JOIN user_film uf ON uf.user_id = %(user_id)s AND uf.film_id = f.film_id
LEFT JOIN last_seen_ev ls ON ls.user_id = %(user_id)s AND ls.film_id = f.film_id
WHERE COALESCE(uf.status::text, 'WANT') <> 'SEEN'
  AND f.title ~ '^[\\x00-\\x7F]+$';
"""

# ===============================
# CORE UTILS
# ===============================

def get_conn():
    return psycopg2.connect(host=PG_HOST, dbname=PG_DB, user=PG_USER, password=PG_PWD, port=PG_PORT)

def fetch_df(conn, sql, params=None):
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute(sql, params or {})
        return pd.DataFrame(cur.fetchall())

def ollama_embed(text: str) -> list[float]:
    r = requests.post(f"{OLLAMA_URL}/api/embeddings", json={"model": EMBED_MODEL, "prompt": text}, timeout=90)
    r.raise_for_status()
    return r.json().get("embedding")

def to_pgvector_literal(vec: list[float]) -> str:
    return "[" + ",".join(f"{float(x):.8f}" for x in vec) + "]"

# ===============================
# LOGIQUE DE FILTRAGE & SCORING
# ===============================

def llm_extract_filters(user_text: str, available_genres: list[str]) -> dict:
    defaults = {"genres_include": [], "max_runtime": None, "top_k": 3}
    prompt = f"Extract movie filters. Genres: {available_genres}. JSON only: {{\"genres_include\": [], \"max_runtime\": integer, \"top_k\": 3}}. Text: {user_text}"
    try:
        r = requests.post(f"{OLLAMA_URL}/api/chat", json={"model": OLLAMA_MODEL, "messages": [{"role": "user", "content": prompt}], "stream": False, "format": "json"}, timeout=15)
        extracted = json.loads(r.json()["message"]["content"])
        defaults.update(extracted)
    except: pass
    return defaults

def apply_filters(candidates, film_genres, filters, genre_id_by_name):
    df = candidates.copy()
    # Filtre Runtime (avec protection contre les strings)
    max_rt = filters.get("max_runtime")
    if max_rt and str(max_rt).isdigit():
        df = df[df["runtime_min"].fillna(999) <= int(max_rt)]
    
    # Filtre Genres
    inc_names = filters.get("genres_include", [])
    inc_ids = [genre_id_by_name[g] for g in inc_names if g in genre_id_by_name]
    if inc_ids:
        df = df[df["film_id"].map(lambda fid: any(gid in film_genres.get(fid, []) for gid in inc_ids))]
    return df

def recommend(candidates, semantic_results, film_genres, genre_profile, film_directors, top_k):
    rows = []
    six_months_ago = pd.Timestamp.now().date() - pd.Timedelta(days=180)
    
    # Map des scores s√©mantiques (synopsis + commentaires)
    sem_map = dict(zip(semantic_results.film_id, semantic_results.semantic_score))
    
    # S√©curit√© : Si aucun r√©sultat s√©mantique n'est trouv√©, on √©vite la division par z√©ro
    max_b = float(max(genre_boost.values())) if 'genre_boost' in locals() and genre_boost else 1.0
    
    for _, f in candidates.iterrows():
        # Filtre 6 mois
        if pd.notnull(f.last_seen_at):
            ls = pd.to_datetime(f.last_seen_at).date()
            if ls > six_months_ago: continue

        genres = film_genres.get(f.film_id, [])
        # Score Profil (Conversion Decimal -> float)
        s_gen = np.mean([float(genre_profile.get(g, 5.0)) for g in genres]) if genres else 5.0
        if np.isnan(s_gen): s_gen = 5.0
        # Score S√©mantique
        s_sem = float(sem_map.get(f.film_id, 0.0)) * 10.0
        if np.isnan(s_sem): s_sem = 0.0
        
        score = (0.3 * s_gen) + (0.7 * s_sem) # Priorit√© √† la recherche actuelle
        
        rows.append({
            "film_id": f.film_id, 
            "title": f.title, 
            "year": f.year, 
            "director": film_directors.get(f.film_id, "Inconnu"),
            "runtime": f.runtime_min, 
            "score": round(min(score, 10.0), 1)
        })
    
    if not rows: 
        return pd.DataFrame(columns=["title", "year", "director", "runtime", "score", "film_id"])
    
    # On s'assure que top_k est un entier valide avant l'addition
    safe_top_k = int(top_k) if top_k is not None else 3
    
    return pd.DataFrame(rows).sort_values("score", ascending=False).head(safe_top_k + 5)

# ===============================
# RE-RANKING & EXPLICATION
# ===============================

def llm_rerank_and_explain(query, candidates_df):
    # On ne passe que l'essentiel au LLM pour √©viter de saturer le contexte
    selection = candidates_df.head(8)[['title', 'director', 'year']].to_dict(orient="records")
    
    prompt = f"""
    En tant qu'expert cin√©ma, choisis les 3 films les plus pertinents parmi cette liste pour r√©pondre √† la recherche : "{query}"
    
    LISTE :
    {json.dumps(selection, ensure_ascii=False)}
    
    CONSIGNES :
    1. R√©ponds UNIQUEMENT avec un tableau JSON.
    2. Format : [ {{"title": "...", "explanation": "..."}}, ... ]
    3. L'explication doit √™tre une phrase unique liant le film √† la recherche "{query}".
    4. Pas de texte avant ou apr√®s le JSON.
    """
    
    try:
        r = requests.post(
            f"{OLLAMA_URL}/api/chat",
            json={
                "model": OLLAMA_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "format": "json", # Force Ollama √† sortir du JSON
                "options": {"temperature": 0.4}
            },
            timeout=120
        )
        r.raise_for_status()
        res = r.json()
        content = res.get("message", {}).get("content", "").strip()
        
        # Nettoyage de s√©curit√©
        if "```" in content:
            content = content.split("```")[1].replace("json", "").strip()
        
        parsed = json.loads(content)
        # On s'assure que c'est bien une liste
        return parsed if isinstance(parsed, list) else []
        
    except Exception as e:
        # On affiche l'erreur en mode debug discret pour comprendre si besoin
        print(f"DEBUG RE-RANK: {e}") 
        return []
    
# ===============================
# INTERFACE
# ===============================
st.set_page_config(page_title="Cin√©ma Hybride", layout="wide")
st.title("üé¨ Assistant Vid√©oth√®que V2")

@st.cache_data(ttl=600)
def load_data(user_id):
    conn = get_conn()
    genres_df = fetch_df(conn, "SELECT genre_id, name FROM genre")
    data = {
        "candidates": fetch_df(conn, SQL_CANDIDATES, {"user_id": user_id}),
        "genre_profile": {int(r.genre_id): float(r.avg_rating) for r in fetch_df(conn, SQL_GENRE_PROFILE, {"user_id": user_id}).itertuples()},
        "film_genres": fetch_df(conn, "SELECT film_id, genre_id FROM film_genre").groupby("film_id")["genre_id"].apply(list).to_dict(),
        "genre_names": dict(zip(genres_df.genre_id, genres_df.name)),
        "genre_id_by_name": dict(zip(genres_df.name, genres_df.genre_id)),
        "directors": dict(zip(*fetch_df(conn, "SELECT fc.film_id, STRING_AGG(p.name, ', ') FROM film_credit fc JOIN person p ON p.person_id = fc.person_id WHERE fc.job = 'Director' GROUP BY fc.film_id").values.T))
    }
    conn.close()
    return data

d = load_data(DEFAULT_USER_ID)

if prompt := st.chat_input("Votre recherche..."):
    with st.chat_message("user"): st.write(prompt)
    
    with st.spinner("Analyse et recherche en cours..."):
        # 1. Extraction des filtres
        filters = llm_extract_filters(prompt, list(d["genre_names"].values()))
        
        # 2. Recherche S√©mantique
        try:
            qvec = to_pgvector_literal(ollama_embed(prompt))
            conn = get_conn()
            semantic_results = fetch_df(conn, SQL_HYBRID_SEARCH, {"user_id": DEFAULT_USER_ID, "qvec": qvec})
            conn.close()
        except Exception as e:
            st.error(f"Erreur recherche s√©mantique : {e}")
            semantic_results = pd.DataFrame()

        # 3. Filtrage & Scoring
        filtered = apply_filters(d["candidates"], d["film_genres"], filters, d["genre_id_by_name"])
        
        # DEBUG : Nombre de films apr√®s filtrage
        # st.write(f"DEBUG: {len(filtered)} films apr√®s filtres.") 

        recos_raw = recommend(filtered, semantic_results, d["film_genres"], d["genre_profile"], d["directors"], filters.get("top_k", 3))

    with st.chat_message("assistant"):
        if recos_raw.empty:
            st.warning("D√©sol√©, je n'ai trouv√© aucun film correspondant √† vos crit√®res (dur√©e, genre, historique). Essayez d'√©largir votre recherche !")
        else:
            with st.spinner("L'assistant affine la s√©lection..."):
                final_items = llm_rerank_and_explain(prompt, recos_raw)
            
            # Si le reranking global a √©chou√© (liste vide), on utilise les scores bruts
            if not final_items:
                # On prend les 3 meilleurs scores math√©matiques
                final_items = [
                    {
                        "title": r.title, 
                        "explanation": f"Ce film de {r.director} ressort comme la meilleure correspondance technique √† votre demande."
                    } 
                    for r in recos_raw.head(3).itertuples()
                ]

            for item in final_items[:3]:
                title = item.get('title')
                # S√©curit√© : On cherche les infos compl√®tes dans recos_raw
                match = recos_raw[recos_raw['title'] == title]
                if not match.empty:
                    info = match.iloc[0]
                    st.markdown(f"### {info.title} ({int(info.year)})")
                    st.caption(f"üé¨ {info.director} | ‚è±Ô∏è {info.runtime} min | ‚≠ê Pertinence: `{info.score}/10`")
                    st.write(f"‚ú® {item.get('explanation')}")
                    st.divider()